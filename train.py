# train.py
import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import os
import time
import random
import copy
from collections import deque

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from src.game import init_game, make_move, game_over
from src.model import ToguzZeroResNet
from src.mcts import MCTS, encode_board

# --- –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø ---
LEARNING_RATE = 0.0001
WEIGHT_DECAY = 1e-4
MCTS_SIMULATIONS = 400   # 400 - —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —É–º–∞
GAMES_PER_LOOP = 20      # –ò–≥—Ä–∞–µ–º 20 –ø–∞—Ä—Ç–∏–π, –ø–æ—Ç–æ–º —É—á–∏–º—Å—è
EPOCHS_PER_LOOP = 5      # –ü—Ä–æ–≥–æ–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ 5 —Ä–∞–∑ —á–µ—Ä–µ–∑ —Å–µ—Ç—å
MEMORY_SIZE = 10000      # –ü–æ–º–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10k —Ö–æ–¥–æ–≤
BATCH_SIZE = 64
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

MODEL_PATH = "models/toguz_best.pth"
CHECKPOINT_PATH = "models/toguz_checkpoint.pth"

def self_play(mcts):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–¥–Ω–æ–π –ø–∞—Ä—Ç–∏–∏ –∏–≥—Ä—ã –º–æ–¥–µ–ª–∏ —Å–∞–º–æ–π —Å —Å–æ–±–æ–π."""
    board, kazans, tuz = init_game()
    player = 0
    history = [] # –°–æ—Ö—Ä–∞–Ω—è–µ–º: (board, kazans, tuz, player, probs)
    
    moves_count = 0
    while not game_over(board, kazans) and moves_count < 250:
        # –ó–∞–ø—É—Å–∫–∞–µ–º MCTS –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        mcts_probs = mcts.search(board, kazans, tuz, player, simulations=MCTS_SIMULATIONS)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        history.append([
            copy.deepcopy(board), 
            copy.deepcopy(kazans), 
            copy.deepcopy(tuz), 
            player, 
            mcts_probs
        ])
        
        # –ü–µ—Ä–≤—ã–µ 30 —Ö–æ–¥–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å (Exploration), 
        # —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –≤–∏–¥–µ–ª–∞ —Ä–∞–∑–Ω—ã–µ –¥–µ–±—é—Ç—ã. –î–∞–ª—å—à–µ - —Å—Ç—Ä–æ–≥–æ (Exploitation).
        if moves_count < 30:
            move = np.random.choice(range(9), p=list(mcts_probs.values()))
        else:
            move = max(mcts_probs, key=mcts_probs.get)
            
        make_move(player, move, board, kazans, tuz)
        player = 1 - player
        moves_count += 1
        
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (1 - –≤—ã–∏–≥—Ä–∞–ª P0, -1 - –≤—ã–∏–≥—Ä–∞–ª P1, 0 - –Ω–∏—á—å—è)
    if kazans[0] > kazans[1]: result = 1.0
    elif kazans[1] > kazans[0]: result = -1.0
    else: result = 0.0
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    processed_data = []
    for h in history:
        h_board, h_kazans, h_tuz, h_player, h_probs = h
        
        # Value Target: –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–≥—Ä—ã —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ –∏–≥—Ä–æ–∫–∞
        # –ï—Å–ª–∏ –≤—ã–∏–≥—Ä–∞–ª P0 (res=1), —Ç–æ –¥–ª—è P0 target=1, –¥–ª—è P1 target=-1
        value_target = result if h_player == 0 else -result
        
        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å probs –≤ —Å–ø–∏—Å–æ–∫ –∏–∑ 9 —á–∏—Å–µ–ª
        policy_target = np.zeros(9, dtype=np.float32)
        for move, prob in h_probs.items():
            policy_target[move] = prob
            
        processed_data.append({
            'board': h_board, 
            'kazans': h_kazans, 
            'tuz': h_tuz, 
            'player': h_player,
            'policy_target': policy_target,
            'value_target': value_target
        })
        
    return processed_data

def train_step(model, optimizer, data_buffer):
    """–û–¥–∏–Ω —à–∞–≥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏."""
    model.train()
    random.shuffle(data_buffer)
    
    total_loss = 0
    batches = 0
    
    for i in range(0, len(data_buffer), BATCH_SIZE):
        batch = data_buffer[i : i + BATCH_SIZE]
        if len(batch) < BATCH_SIZE // 2: break 
        
        x_list, k_list, p_target_list, v_target_list = [], [], [], []
        
        for item in batch:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–Ω–∑–æ—Ä–æ–≤
            x, k = encode_board(item['player'], item['board'], item['kazans'], item['tuz'], DEVICE)
            x_list.append(x)
            k_list.append(k)
            p_target_list.append(torch.tensor(item['policy_target'], dtype=torch.float32))
            v_target_list.append(torch.tensor([item['value_target']], dtype=torch.float32))
            
        x_batch = torch.cat(x_list).to(DEVICE)
        k_batch = torch.cat(k_list).to(DEVICE)
        p_target = torch.stack(p_target_list).to(DEVICE)
        v_target = torch.stack(v_target_list).to(DEVICE)
        
        optimizer.zero_grad()
        p_pred, v_pred = model(x_batch, k_batch)
        
        # 1. Value Loss (MSE): –ù–∞—Å–∫–æ–ª—å–∫–æ –º—ã –æ—à–∏–±–ª–∏—Å—å –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è?
        loss_v = F.mse_loss(v_pred, v_target)
        
        # 2. Policy Loss (CrossEntropy): –ù–∞—Å–∫–æ–ª—å–∫–æ –Ω–∞—à–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç MCTS?
        log_probs = F.log_softmax(p_pred, dim=1)
        loss_p = -torch.mean(torch.sum(p_target * log_probs, dim=1))
        
        # –°—É–º–º–∞ –æ—à–∏–±–æ–∫
        loss = loss_v + loss_p
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        batches += 1
        
    return total_loss / (batches + 1e-8)

def run_training():
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è ToguzZero (Device: {DEVICE})")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É models, –µ—Å–ª–∏ –Ω–µ—Ç
    os.makedirs("models", exist_ok=True)
    
    model = ToguzZeroResNet(num_res_blocks=5).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
    start_loop = 1
    if os.path.exists(MODEL_PATH):
        print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {MODEL_PATH}")
        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    else:
        print("üÜï –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è.")

    mcts = MCTS(model, DEVICE)
    replay_buffer = deque(maxlen=MEMORY_SIZE)
    
    print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: MCTS_Sims={MCTS_SIMULATIONS}, Games_Per_Loop={GAMES_PER_LOOP}")
    
    try:
        loop = 0
        while True:
            loop += 1
            start_time = time.time()
            
            # 1. Self-Play
            print(f"[Loop {loop}] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∞—Ä—Ç–∏–π...", end=" ", flush=True)
            model.eval()
            new_data = []
            for _ in range(GAMES_PER_LOOP):
                game_data = self_play(mcts)
                new_data.extend(game_data)
                print(".", end="", flush=True)
            
            replay_buffer.extend(new_data)
            print(f" Done. Buffer: {len(replay_buffer)}")
            
            # 2. Training
            if len(replay_buffer) >= BATCH_SIZE:
                print(f"[Loop {loop}] –û–±—É—á–µ–Ω–∏–µ...", end=" ")
                avg_loss = 0
                for _ in range(EPOCHS_PER_LOOP):
                    loss = train_step(model, optimizer, list(replay_buffer))
                    avg_loss += loss
                
                print(f"Loss: {avg_loss/EPOCHS_PER_LOOP:.4f}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
                torch.save(model.state_dict(), MODEL_PATH)
                # –ú–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏
                # torch.save(model.state_dict(), f"models/toguz_loop_{loop}.pth")
            
            print(f"–í—Ä–µ–º—è —Ü–∏–∫–ª–∞: {time.time() - start_time:.1f} —Å–µ–∫.\n")
            
    except KeyboardInterrupt:
        print("\nüõë –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
        print("–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
        torch.save(model.state_dict(), MODEL_PATH)

if __name__ == "__main__":
    run_training()